{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tratamento de warning\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r datatran"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = datatran"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise da causa do acidente, coluna de texto através de Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Ausência de reação do condutor', 'Entrada inopinada do pedestre',\n",
       "       'Reação tardia ou ineficiente do condutor',\n",
       "       'Velocidade Incompatível', 'Acumulo de água sobre o pavimento',\n",
       "       'Condutor Dormindo', 'Desrespeitar a preferência no cruzamento',\n",
       "       'Demais falhas mecânicas ou elétricas', 'Transitar na contramão',\n",
       "       'Acessar a via sem observar a presença dos outros veículos',\n",
       "       'Manobra de mudança de faixa',\n",
       "       'Avarias e/ou desgaste excessivo no pneu', 'Pista Escorregadia',\n",
       "       'Condutor deixou de manter distância do veículo da frente',\n",
       "       'Trafegar com motocicleta (ou similar) entre as faixas',\n",
       "       'Pista esburacada', 'Pedestre andava na pista',\n",
       "       'Acumulo de óleo sobre o pavimento',\n",
       "       'Carga excessiva e/ou mal acondicionada',\n",
       "       'Condutor desrespeitou a iluminação vermelha do semáforo', 'Chuva',\n",
       "       'Mal súbito do condutor', 'Curva acentuada', 'Obstrução na via',\n",
       "       'Ultrapassagem Indevida', 'Afundamento ou ondulação no pavimento',\n",
       "       'Frear bruscamente', 'Problema com o freio',\n",
       "       'Demais falhas na via', 'Animais na Pista',\n",
       "       'Problema na suspensão', 'Ingestão de álcool pelo condutor',\n",
       "       'Conversão proibida', 'Pedestre cruzava a pista fora da faixa',\n",
       "       'Restrição de visibilidade em curvas horizontais',\n",
       "       'Acostamento em desnível',\n",
       "       'Falta de elemento de contenção que evite a saída do leito carroçável',\n",
       "       'Estacionar ou parar em local proibido', 'Ausência de sinalização',\n",
       "       'Obras na pista', 'Acesso irregular', 'Desvio temporário',\n",
       "       'Ingestão de álcool e/ou substâncias psicoativas pelo pedestre',\n",
       "       'Ingestão de substâncias psicoativas pelo condutor',\n",
       "       'Retorno proibido', 'Condutor usando celular', 'Neblina',\n",
       "       'Falta de acostamento', 'Redutor de velocidade em desacordo',\n",
       "       'Transitar na calçada',\n",
       "       'Acumulo de areia ou detritos sobre o pavimento',\n",
       "       'Modificação proibida', 'Sinalização mal posicionada',\n",
       "       'Pista em desnível', 'Objeto estático sobre o leito carroçável',\n",
       "       'Iluminação deficiente',\n",
       "       'Ingestão de álcool ou de substâncias psicoativas pelo pedestre',\n",
       "       'Deficiência do Sistema de Iluminação/Sinalização',\n",
       "       'Demais Fenômenos da natureza', 'Declive acentuado',\n",
       "       'Obstrução Via tentativa Assalto',\n",
       "       'Deixar de acionar o farol da motocicleta (ou similar)', 'Fumaça',\n",
       "       'Faixas de trânsito com largura insuficiente',\n",
       "       'Sinalização encoberta', 'Transitar no Acostamento',\n",
       "       'Restrição de visibilidade em curvas verticais',\n",
       "       'Área urbana sem a presença de local apropriado para a travessia de pedestres',\n",
       "       'Suicídio (presumido)', 'Transtornos Mentais (exceto suicidio)',\n",
       "       'Sistema de drenagem ineficiente', 'Participar de racha',\n",
       "       'Faróis desregulados',\n",
       "       'Pedestre - Ingestão de álcool/ substâncias psicoativas',\n",
       "       'Semáforo com defeito'], dtype=object)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['causa_acidente'].unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "40481143-703f-4ca3-8f35-edfdfba63f31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Tokenização para remoção de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Baixar dados necessários do NLTK\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_stopwords = set(stopwords.words('portuguese')) | set([\n",
    "    \" \", \"ou\", \"\", \"ok\", \"problema\", \"paciente\", \"necessário\", \"feito\", \"w\", \"devido\",\n",
    "    \"precisa\", \"dr\", \"opções\", \"caso\", \"o\", \"x\", \"tentar\", \"ainda\", \"próximo\", \"r\",\n",
    "    \"d\", \"desde\", \"teste\", \"testando\", \"resultados\", \"recomendar\", \"pode\", \"por favor\",\n",
    "    \"médico\", \"normal\", \"seria\", \"discutido\", \"medicamento\", \"doença\",\n",
    "    \"sugerido\", \"considerar\", \"sim\", \"provável\", \"clínico\", \"revisão\",\n",
    "    \"interno\", \"tratar\", \"rever\", \"semana\", \"ensaio\", \"comentário\", \"rec\",\n",
    "    \"vai\", \"nós\", \"oi\", \"olá\", \"cumprimento\", \"gostar\", \"saber\", \"sim\", \"certo\",\n",
    "    \"amanhã\", \"olhar\", \"dizer\", \"okay\", \"quilograma\", \"zoom\", \"link\", \"tudo bem\"\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenizar e remover stopwords\n",
    "def process_text(text):\n",
    "        tokens = word_tokenize(str(text).lower())  # Tokenizar e converter para minúsculas\n",
    "        filtered_tokens = [word for word in tokens if word.isalnum() and word not in custom_stopwords]\n",
    "        return \" \".join(filtered_tokens)  # Reunir palavras filtradas em uma string\n",
    "\n",
    "# Aplicar processamento ao DataFrame\n",
    "df[\"palavras_filtradas\"] = df[\"causa_acidente\"].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ano</th>\n",
       "      <th>feriado</th>\n",
       "      <th>mes</th>\n",
       "      <th>data_inversa</th>\n",
       "      <th>uf</th>\n",
       "      <th>br</th>\n",
       "      <th>km</th>\n",
       "      <th>municipio</th>\n",
       "      <th>causa_acidente</th>\n",
       "      <th>...</th>\n",
       "      <th>fase_dia</th>\n",
       "      <th>dia_semana</th>\n",
       "      <th>tipo_acidente</th>\n",
       "      <th>tipo_pista</th>\n",
       "      <th>mortos</th>\n",
       "      <th>feridos_graves</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>status</th>\n",
       "      <th>palavras_filtradas</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>496519.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>ES</td>\n",
       "      <td>101</td>\n",
       "      <td>114</td>\n",
       "      <td>SOORETAMA</td>\n",
       "      <td>Ausência de reação do condutor</td>\n",
       "      <td>...</td>\n",
       "      <td>Plena Noite</td>\n",
       "      <td>domingo</td>\n",
       "      <td>Saída de leito carroçável</td>\n",
       "      <td>Simples</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>-19,09484877</td>\n",
       "      <td>-40,05095848</td>\n",
       "      <td>Segura</td>\n",
       "      <td>ausência reação condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>496543.0</td>\n",
       "      <td>2023</td>\n",
       "      <td>y</td>\n",
       "      <td>1</td>\n",
       "      <td>2023-01-01</td>\n",
       "      <td>SP</td>\n",
       "      <td>116</td>\n",
       "      <td>113,1</td>\n",
       "      <td>TAUBATE</td>\n",
       "      <td>Entrada inopinada do pedestre</td>\n",
       "      <td>...</td>\n",
       "      <td>Plena Noite</td>\n",
       "      <td>domingo</td>\n",
       "      <td>Atropelamento de Pedestre</td>\n",
       "      <td>Dupla</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>-23,0445658</td>\n",
       "      <td>-45,58259814</td>\n",
       "      <td>Perigosa</td>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id   ano feriado  mes data_inversa  uf   br     km  municipio  \\\n",
       "0  496519.0  2023       y    1   2023-01-01  ES  101    114  SOORETAMA   \n",
       "1  496543.0  2023       y    1   2023-01-01  SP  116  113,1    TAUBATE   \n",
       "\n",
       "                   causa_acidente  ...     fase_dia  dia_semana  \\\n",
       "0  Ausência de reação do condutor  ...  Plena Noite     domingo   \n",
       "1   Entrada inopinada do pedestre  ...  Plena Noite     domingo   \n",
       "\n",
       "               tipo_acidente tipo_pista mortos feridos_graves      latitude  \\\n",
       "0  Saída de leito carroçável    Simples      0              0  -19,09484877   \n",
       "1  Atropelamento de Pedestre      Dupla      1              0   -23,0445658   \n",
       "\n",
       "      longitude    status          palavras_filtradas  \n",
       "0  -40,05095848    Segura    ausência reação condutor  \n",
       "1  -45,58259814  Perigosa  entrada inopinada pedestre  \n",
       "\n",
       "[2 rows x 23 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lematização e Radicalização\n",
    "\n",
    "🔹 Lema (Lemmatization): Processo de reduzir palavras à sua forma base ou raiz, conhecida como lema.\n",
    "\n",
    "🔹 Radical (Stemming): Redução de palavras à sua forma raiz (stem) sem considerar regras gramaticais ou significado.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.corpus import wordnet\n",
    "\n",
    "def lemma_nltk(df):\n",
    "    # Initialize the lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "    def get_wordnet_pos(word):\n",
    "        \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "        tag = nltk.pos_tag([word])[0][1][0].upper()\n",
    "        tag_dict = {\"J\": wordnet.ADJ,\n",
    "                    \"N\": wordnet.NOUN,\n",
    "                    \"V\": wordnet.VERB,\n",
    "                    \"R\": wordnet.ADV}\n",
    "        return tag_dict.get(tag, wordnet.NOUN)\n",
    "\n",
    "    def lemmatize_words(text):\n",
    "        words = text.split()\n",
    "        lemmatized_words = [lemmatizer.lemmatize(word, get_wordnet_pos(word)) for word in words]\n",
    "        return ' '.join(lemmatized_words)\n",
    "\n",
    "    # Apply the lemmatization function to the DataFrame column\n",
    "    df['lemma'] = df['palavras_filtradas'].apply(lemmatize_words)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 4min 31s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_lemma = lemma_nltk(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palavras_filtradas</th>\n",
       "      <th>lemma</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ausência reação condutor</td>\n",
       "      <td>ausência reação condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reação tardia ineficiente condutor</td>\n",
       "      <td>reação tardia ineficiente condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>velocidade incompatível</td>\n",
       "      <td>velocidade incompatível</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acumulo água sobre pavimento</td>\n",
       "      <td>acumulo água sobre pavimento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>condutor dormindo</td>\n",
       "      <td>condutor dormindo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>desrespeitar preferência cruzamento</td>\n",
       "      <td>desrespeitar preferência cruzamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>demais falhas mecânicas elétricas</td>\n",
       "      <td>demais falhas mecânicas elétricas</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>transitar contramão</td>\n",
       "      <td>transitar contramão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>velocidade incompatível</td>\n",
       "      <td>velocidade incompatível</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    palavras_filtradas                                lemma\n",
       "0             ausência reação condutor             ausência reação condutor\n",
       "1           entrada inopinada pedestre           entrada inopinada pedestre\n",
       "2   reação tardia ineficiente condutor   reação tardia ineficiente condutor\n",
       "3              velocidade incompatível              velocidade incompatível\n",
       "4         acumulo água sobre pavimento         acumulo água sobre pavimento\n",
       "5                    condutor dormindo                    condutor dormindo\n",
       "6  desrespeitar preferência cruzamento  desrespeitar preferência cruzamento\n",
       "7    demais falhas mecânicas elétricas    demais falhas mecânicas elétricas\n",
       "8                  transitar contramão                  transitar contramão\n",
       "9              velocidade incompatível              velocidade incompatível"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lemma[['palavras_filtradas', 'lemma']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "def stem_nltk_after_lemma(df):\n",
    "    # Initialize the stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "\n",
    "    def stem_words(text):\n",
    "        # Split the text into words\n",
    "        words = text.split()\n",
    "        # Stem each word\n",
    "        stemmed_words = [stemmer.stem(word) for word in words]\n",
    "        # Join the stemmed words back into a single string\n",
    "        return ' '.join(stemmed_words)\n",
    "\n",
    "    # Apply the stemming function to the DataFrame\n",
    "    df['normalizado'] = df['lemma'].apply(stem_words)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 9.93 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_final = stem_nltk_after_lemma(df_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>palavras_filtradas</th>\n",
       "      <th>lemma</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ausência reação condutor</td>\n",
       "      <td>ausência reação condutor</td>\n",
       "      <td>ausência reação condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "      <td>entrada inopinada pedestre</td>\n",
       "      <td>entrada inopinada pedestr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>reação tardia ineficiente condutor</td>\n",
       "      <td>reação tardia ineficiente condutor</td>\n",
       "      <td>reação tardia ineficient condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>velocidade incompatível</td>\n",
       "      <td>velocidade incompatível</td>\n",
       "      <td>velocidad incompatível</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>acumulo água sobre pavimento</td>\n",
       "      <td>acumulo água sobre pavimento</td>\n",
       "      <td>acumulo água sobr pavimento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>condutor dormindo</td>\n",
       "      <td>condutor dormindo</td>\n",
       "      <td>condutor dormindo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>desrespeitar preferência cruzamento</td>\n",
       "      <td>desrespeitar preferência cruzamento</td>\n",
       "      <td>desrespeitar preferência cruzamento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>demais falhas mecânicas elétricas</td>\n",
       "      <td>demais falhas mecânicas elétricas</td>\n",
       "      <td>demai falha mecânica elétrica</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>transitar contramão</td>\n",
       "      <td>transitar contramão</td>\n",
       "      <td>transitar contramão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>velocidade incompatível</td>\n",
       "      <td>velocidade incompatível</td>\n",
       "      <td>velocidad incompatível</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    palavras_filtradas                                lemma  \\\n",
       "0             ausência reação condutor             ausência reação condutor   \n",
       "1           entrada inopinada pedestre           entrada inopinada pedestre   \n",
       "2   reação tardia ineficiente condutor   reação tardia ineficiente condutor   \n",
       "3              velocidade incompatível              velocidade incompatível   \n",
       "4         acumulo água sobre pavimento         acumulo água sobre pavimento   \n",
       "5                    condutor dormindo                    condutor dormindo   \n",
       "6  desrespeitar preferência cruzamento  desrespeitar preferência cruzamento   \n",
       "7    demais falhas mecânicas elétricas    demais falhas mecânicas elétricas   \n",
       "8                  transitar contramão                  transitar contramão   \n",
       "9              velocidade incompatível              velocidade incompatível   \n",
       "\n",
       "                           normalizado  \n",
       "0             ausência reação condutor  \n",
       "1            entrada inopinada pedestr  \n",
       "2    reação tardia ineficient condutor  \n",
       "3               velocidad incompatível  \n",
       "4          acumulo água sobr pavimento  \n",
       "5                    condutor dormindo  \n",
       "6  desrespeitar preferência cruzamento  \n",
       "7        demai falha mecânica elétrica  \n",
       "8                  transitar contramão  \n",
       "9               velocidad incompatível  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[['palavras_filtradas', 'lemma', 'normalizado']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>ano</th>\n",
       "      <th>feriado</th>\n",
       "      <th>mes</th>\n",
       "      <th>data_inversa</th>\n",
       "      <th>uf</th>\n",
       "      <th>br</th>\n",
       "      <th>km</th>\n",
       "      <th>municipio</th>\n",
       "      <th>causa_acidente</th>\n",
       "      <th>...</th>\n",
       "      <th>tipo_acidente</th>\n",
       "      <th>tipo_pista</th>\n",
       "      <th>mortos</th>\n",
       "      <th>feridos_graves</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>status</th>\n",
       "      <th>palavras_filtradas</th>\n",
       "      <th>lemma</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows × 25 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [id, ano, feriado, mes, data_inversa, uf, br, km, municipio, causa_acidente, classificacao_acidente, veiculos, condicao_metereologica, fase_dia, dia_semana, tipo_acidente, tipo_pista, mortos, feridos_graves, latitude, longitude, status, palavras_filtradas, lemma, normalizado]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 25 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['lemma']==''].head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LDA (entendendo o algoritmo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "import gensim\n",
    "from gensim.models import CoherenceModel\n",
    "from gensim.corpora.dictionary import Dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def lda(df_final, topics, words, threshold):\n",
    "    \"\"\"\n",
    "    Executa a **Latent Dirichlet Allocation (LDA)** em um DataFrame para identificar tópicos.\n",
    "\n",
    "    **Parâmetros:**\n",
    "    df_final (DataFrame): O DataFrame de entrada contendo os dados de texto.\n",
    "    topics (int): O número de tópicos a identificar.\n",
    "    words (int): O número de palavras mais relevantes a serem exibidas para cada tópico.\n",
    "\n",
    "    **Retorna:**\n",
    "    df_topics (DataFrame): O DataFrame de saída contendo os dados de texto.\n",
    "\n",
    "    Esta função divide o DataFrame em conjuntos de treino e teste, **vetoriza os dados de texto** usando **TF-IDF**, \n",
    "    ajusta um modelo LDA nos dados de treino e transforma os dados de teste. \n",
    "    Ela imprime o número de linhas nos conjuntos de treino e teste, confirma a criação das matrizes correspondentes, \n",
    "    e indica a conclusão das fases de treinamento e teste.\n",
    "    \"\"\"\n",
    "\n",
    "    # Separar o DataFrame em conjuntos de treino e teste\n",
    "    train_df = df_final.sample(frac=0.7, random_state=42)\n",
    "    test_df = df_final.drop(train_df.index)\n",
    "\n",
    "    print(f\"Contagem df treino: {len(train_df)}\")\n",
    "    print(f\"Contagem df teste: {len(test_df)}\")\n",
    "\n",
    "    train_texts = train_df['normalizado'].tolist()\n",
    "    test_texts = test_df['normalizado'].tolist()\n",
    "\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    train_matrix = vectorizer.fit_transform(train_texts)\n",
    "    test_matrix = vectorizer.transform(test_texts)\n",
    "\n",
    "    print(f\"Matrix de treino e testes criadas!\")\n",
    "\n",
    "    # Define o número de tópicos e palavras dentro de cada tópico\n",
    "    num_topics = topics\n",
    "    num_top_words = words\n",
    "\n",
    "    # Criação LDA object\n",
    "    lda = LatentDirichletAllocation(doc_topic_prior=0.5, learning_decay=0.5, learning_method='online', max_iter= 10, topic_word_prior=0.1, n_components=num_topics, random_state=42)\n",
    "\n",
    "    # Fit do modelo na matrix de treino\n",
    "    lda_matrix_train = lda.fit_transform(train_matrix)\n",
    "\n",
    "    print(f\"Treinamento completo!\")\n",
    "\n",
    "    # Transforma a matrix de teste\n",
    "    lda_matrix_test = lda.transform(test_matrix)\n",
    "\n",
    "    print(f\"Teste completo!\")\n",
    "\n",
    "    # Obtém os termos de cada tópico\n",
    "    terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "    topics = []\n",
    "    top_terms_dict = {}\n",
    "    for idx, topic in enumerate(lda.components_):\n",
    "        top_terms = [terms[i] for i in topic.argsort()[-num_top_words:]]\n",
    "        topics.append((f\"Topic {idx + 1}\", \", \".join(top_terms)))\n",
    "        top_terms_dict[idx] = \", \".join(top_terms)\n",
    "\n",
    "    # Criação df com os tópicos\n",
    "    df_topics = pd.DataFrame(topics, columns=[\"Topic\", \"Top Terms\"])\n",
    "\n",
    "    print(f\"Tópicos gerados!\")\n",
    "\n",
    "    # Teste de perplexidade\n",
    "    perplexity = lda.perplexity(test_matrix)\n",
    "    print(f'Perplexity: {perplexity}')\n",
    " \n",
    "    # Prepara o dado para coherence model\n",
    "    texts = [doc.split() for doc in test_texts]\n",
    "    dictionary = Dictionary(texts)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "\n",
    "    # Fit do modelo LDA usando gensim\n",
    "    lda_model = gensim.models.LdaModel(corpus, num_topics=num_topics, id2word=dictionary, passes=10)\n",
    "\n",
    "    # Cálculo coherence score\n",
    "    coherence_model_lda = CoherenceModel(model=lda_model, texts=texts, dictionary=dictionary, coherence='c_v')\n",
    "    coherence_score = coherence_model_lda.get_coherence()\n",
    "    print(f'Coherence Score: {coherence_score}')\n",
    "\n",
    "    # Atribue os tópicos para o df original\n",
    "    # O parâmetro de limite (threshold) é utilizado para garantir que apenas tópicos com uma probabilidade acima do limite especificado sejam atribuídos. Se a maior probabilidade estiver abaixo desse limite, o tópico é definido como -1, indicando que não há uma atribuição clara de tópico. Essa abordagem pode ajudar a melhorar a precisão do mapeamento de tópicos.\n",
    "    num_threshold = threshold\n",
    "    topic_prob_matrix = lda.transform(vectorizer.transform(df_final['normalizado'].tolist()))\n",
    "    df_final['Topic'] = np.where(topic_prob_matrix.max(axis=1) >= num_threshold, topic_prob_matrix.argmax(axis=1), -1)\n",
    "    df_final['Top Terms'] = df_final['Topic'].map(lambda x: top_terms_dict.get(x, ''))\n",
    "\n",
    "    return df_topics, df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem df treino: 114395\n",
      "Contagem df teste: 49026\n",
      "Matrix de treino e testes criadas!\n",
      "Treinamento completo!\n",
      "Teste completo!\n",
      "Tópicos gerados!\n",
      "Perplexity: 44.78924551228605\n",
      "Coherence Score: 0.5328602001293569\n",
      "Wall time: 2min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_topics, df_final = lda(df_final, 8, 4, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>contramão, transitar, incompatível, velocidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>pedestr, animai, chuva, pista</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>dormindo, álcool, ingestão, condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>observar, presença, via, veículo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>ineficient, ausência, condutor, reação</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>acostamento, acumulo, pavimento, sobr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>6</td>\n",
       "      <td>mecânica, elétrica, falha, demai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>7</td>\n",
       "      <td>frear, manobra, mudança, faixa</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic                                      Top Terms\n",
       "3       0  contramão, transitar, incompatível, velocidad\n",
       "1       1                  pedestr, animai, chuva, pista\n",
       "5       2           dormindo, álcool, ingestão, condutor\n",
       "11      3               observar, presença, via, veículo\n",
       "0       4         ineficient, ausência, condutor, reação\n",
       "4       5          acostamento, acumulo, pavimento, sobr\n",
       "7       6               mecânica, elétrica, falha, demai\n",
       "17      7                 frear, manobra, mudança, faixa"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[['Topic', 'Top Terms']].drop_duplicates().sort_values(by='Topic', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topico_para_categoria = {\n",
    "    0: \"Infração de Trânsito\",\n",
    "    1: \"Obstáculo na pista ou pedestre\",\n",
    "    2: \"Condutor sob efeitos de substâncias\",\n",
    "    3: \"Manobra imprudente condutor\",\n",
    "    4: \"Manobra imprudente condutor\",\n",
    "    5: \"Acostamento\",\n",
    "    6: \"Problema mecânico no veículo\",\n",
    "    7: \"Manobra imprudente condutor\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explorando os tópicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>causa_acidente</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Acessar a via sem observar a presença dos outros veículos</td>\n",
       "      <td>3</td>\n",
       "      <td>observar, presença, via, veículo</td>\n",
       "      <td>acessar via observar presença outro veículo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               causa_acidente  Topic  \\\n",
       "11  Acessar a via sem observar a presença dos outros veículos      3   \n",
       "\n",
       "                           Top Terms  \\\n",
       "11  observar, presença, via, veículo   \n",
       "\n",
       "                                    normalizado  \n",
       "11  acessar via observar presença outro veículo  "
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['normalizado'].str.contains('observar', na=False)][['causa_acidente','Topic', 'Top Terms', 'normalizado']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>ineficient, ausência, condutor, reação</td>\n",
       "      <td>reação tardia ineficient condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12502</th>\n",
       "      <td>7</td>\n",
       "      <td>frear, manobra, mudança, faixa</td>\n",
       "      <td>sistema drenagem ineficient</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Topic                               Top Terms  \\\n",
       "2          4  ineficient, ausência, condutor, reação   \n",
       "12502      7          frear, manobra, mudança, faixa   \n",
       "\n",
       "                             normalizado  \n",
       "2      reação tardia ineficient condutor  \n",
       "12502        sistema drenagem ineficient  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['normalizado'].str.contains('ineficient', na=False)][['Topic', 'Top Terms', 'normalizado']].drop_duplicates()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analisando Medidas de Acurácia"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Perplexity\n",
    "A perplexidade é uma medida de quão bem um modelo probabilístico prevê um conjunto de amostras. No contexto de LDA (Latent Dirichlet Allocation), valores mais baixos de perplexidade indicam um melhor ajuste aos dados.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Coherence Score\n",
    "A coerência mede a similaridade semântica entre as palavras mais relevantes dentro dos tópicos. Pontuações mais altas indicam tópicos de melhor qualidade. \n",
    "\n",
    "Um Coherence Score de 0.5329 indica que os tópicos gerados pelo modelo LDA possuem um nível razoável de interpretabilidade e similaridade semântica.\n",
    "Interpretação da pontuação:\n",
    "\n",
    "- Acima de 0.5 → Indica que os tópicos têm uma coerência moderada a boa, com palavras que fazem sentido juntas.\n",
    "- Entre 0.6 e 0.8 → Indica boa coerência semântica, sugerindo que os tópicos estão bem agrupados e interpretáveis.\n",
    "- Acima de 0.8 → Indica alta qualidade, onde os tópicos gerados são claramente diferenciáveis e têm forte significado semântico.\n",
    "\n",
    "🔹 Resultado (0.5329) sugere que os tópicos fazem sentido, mas ainda há espaço para refinamento. Algumas melhorias que podem ser feitas para aumentar a coerência:\n",
    "\n",
    "- Refinar o pré-processamento do texto → Remover palavras irrelevantes ou normalizar melhor o texto.\n",
    "- Ajustar o número de tópicos → Talvez reduzir ou aumentar um pouco os tópicos ajude na segmentação das palavras.\n",
    "- Alterar hiperparâmetros do LDA → Testar ajustes na alpha e beta, além de aumentar o número de iterações.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRID SEARCH \n",
    "\n",
    ">- simplificado devido à problemas de performance do micro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils.grid_search import grid_search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Contagem df treino: 114395\n",
      "Contagem df teste: 49026\n",
      "Matrix de treino e testes criadas!\n",
      "Fitting 3 folds for each of 32 candidates, totalling 96 fits\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  39.2s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  41.8s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  27.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  30.7s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  29.9s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  30.0s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  30.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  46.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  47.3s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  47.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  28.3s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  42.8s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.1min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.3min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.3min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.4min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  60.0s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  44.1s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  43.9s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  44.3s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  44.0s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  45.7s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  44.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  44.9s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  46.9s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  46.5s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  45.2s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  44.7s\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.1min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time= 1.1min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.1, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time= 1.0min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  59.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time= 1.0min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time= 1.0min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  59.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  59.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  58.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  58.3s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  58.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  45.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  39.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  39.3s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  57.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  57.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  58.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  59.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  57.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  57.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  57.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time= 1.2min\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  56.3s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  56.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  56.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.3, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  55.9s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  40.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  39.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.01; total time=  39.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  39.7s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  39.8s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=5, topic_word_prior=0.1; total time=  39.7s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  39.9s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  38.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.01; total time=  39.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  39.1s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  38.9s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=5, n_components=8, topic_word_prior=0.1; total time=  38.9s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  56.5s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  56.8s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.01; total time=  56.4s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  56.3s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  56.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=5, topic_word_prior=0.1; total time=  58.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  56.8s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  56.8s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.01; total time=  56.6s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  57.0s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  56.2s\n",
      "[CV] END doc_topic_prior=0.5, learning_decay=0.5, learning_method=batch, max_iter=10, n_components=8, topic_word_prior=0.1; total time=  57.2s\n",
      "Best Parameters: {'doc_topic_prior': 0.1, 'learning_decay': 0.3, 'learning_method': 'batch', 'max_iter': 10, 'n_components': 5, 'topic_word_prior': 0.01}\n",
      "Teste completo!\n",
      "Tópicos gerados!\n",
      "Perplexity: 34.17775315164295\n",
      "Coherence Score: 0.5742541198565523\n",
      "Wall time: 1h 28min 47s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "df_topics, df_final = grid_search(df_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contramão, incompatível, velocidad</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>avaria, pneu, pedestr, animai, chuva, pista, elétrica, mecânica, falha, demai</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>cruzamento, desrespeitar, preferência, dormindo, condutor, faixa, manobra, mudança, álcool, ingestão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>3</td>\n",
       "      <td>frent, distância, manter, deixou, via, presença, acessar, observar, outro, veículo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>fenômeno, usando, celular, suicídio, presumido, tardia, ineficient, ausência, condutor, reação</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Topic  \\\n",
       "3       0   \n",
       "1       1   \n",
       "5       2   \n",
       "11      3   \n",
       "0       4   \n",
       "\n",
       "                                                                                                           Top Terms  \n",
       "3   proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contramão, incompatível, velocidad  \n",
       "1                                      avaria, pneu, pedestr, animai, chuva, pista, elétrica, mecânica, falha, demai  \n",
       "5               cruzamento, desrespeitar, preferência, dormindo, condutor, faixa, manobra, mudança, álcool, ingestão  \n",
       "11                                frent, distância, manter, deixou, via, presença, acessar, observar, outro, veículo  \n",
       "0                     fenômeno, usando, celular, suicídio, presumido, tardia, ineficient, ausência, condutor, reação  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[['Topic', 'Top Terms']].drop_duplicates().sort_values(by='Topic', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "topico_para_categoria = {\n",
    "    0: \"Infração de Trânsito\",\n",
    "    1: \"Problema mecânico no veículo\",\n",
    "    2: \"Condutor sob efeitos de substâncias\",\n",
    "    3: \"Manobra imprudente condutor\",\n",
    "    4: \"Manobra imprudente condutor\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Categoria'] = df_final['Topic'].map(topico_para_categoria)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Categoria</th>\n",
       "      <th>Topic</th>\n",
       "      <th>Top Terms</th>\n",
       "      <th>normalizado</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>4</td>\n",
       "      <td>fenômeno, usando, celular, suicídio, presumido, tardia, ineficient, ausência, condutor, reação</td>\n",
       "      <td>ausência reação condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Problema mecânico no veículo</td>\n",
       "      <td>1</td>\n",
       "      <td>avaria, pneu, pedestr, animai, chuva, pista, elétrica, mecânica, falha, demai</td>\n",
       "      <td>entrada inopinada pedestr</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>4</td>\n",
       "      <td>fenômeno, usando, celular, suicídio, presumido, tardia, ineficient, ausência, condutor, reação</td>\n",
       "      <td>reação tardia ineficient condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Infração de Trânsito</td>\n",
       "      <td>0</td>\n",
       "      <td>proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contramão, incompatível, velocidad</td>\n",
       "      <td>velocidad incompatível</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Problema mecânico no veículo</td>\n",
       "      <td>1</td>\n",
       "      <td>avaria, pneu, pedestr, animai, chuva, pista, elétrica, mecânica, falha, demai</td>\n",
       "      <td>acumulo água sobr pavimento</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163416</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>4</td>\n",
       "      <td>fenômeno, usando, celular, suicídio, presumido, tardia, ineficient, ausência, condutor, reação</td>\n",
       "      <td>reação tardia ineficient condutor</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163417</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>3</td>\n",
       "      <td>frent, distância, manter, deixou, via, presença, acessar, observar, outro, veículo</td>\n",
       "      <td>acessar via observar presença outro veículo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163418</th>\n",
       "      <td>Infração de Trânsito</td>\n",
       "      <td>0</td>\n",
       "      <td>proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contramão, incompatível, velocidad</td>\n",
       "      <td>transitar contramão</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163419</th>\n",
       "      <td>Manobra imprudente condutor</td>\n",
       "      <td>3</td>\n",
       "      <td>frent, distância, manter, deixou, via, presença, acessar, observar, outro, veículo</td>\n",
       "      <td>condutor deixou manter distância veículo frent</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>163420</th>\n",
       "      <td>Condutor sob efeitos de substâncias</td>\n",
       "      <td>2</td>\n",
       "      <td>cruzamento, desrespeitar, preferência, dormindo, condutor, faixa, manobra, mudança, álcool, ingestão</td>\n",
       "      <td>condutor dormindo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>163421 rows × 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  Categoria  Topic  \\\n",
       "0               Manobra imprudente condutor      4   \n",
       "1              Problema mecânico no veículo      1   \n",
       "2               Manobra imprudente condutor      4   \n",
       "3                      Infração de Trânsito      0   \n",
       "4              Problema mecânico no veículo      1   \n",
       "...                                     ...    ...   \n",
       "163416          Manobra imprudente condutor      4   \n",
       "163417          Manobra imprudente condutor      3   \n",
       "163418                 Infração de Trânsito      0   \n",
       "163419          Manobra imprudente condutor      3   \n",
       "163420  Condutor sob efeitos de substâncias      2   \n",
       "\n",
       "                                                                                                               Top Terms  \\\n",
       "0                         fenômeno, usando, celular, suicídio, presumido, tardia, ineficient, ausência, condutor, reação   \n",
       "1                                          avaria, pneu, pedestr, animai, chuva, pista, elétrica, mecânica, falha, demai   \n",
       "2                         fenômeno, usando, celular, suicídio, presumido, tardia, ineficient, ausência, condutor, reação   \n",
       "3       proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contramão, incompatível, velocidad   \n",
       "4                                          avaria, pneu, pedestr, animai, chuva, pista, elétrica, mecânica, falha, demai   \n",
       "...                                                                                                                  ...   \n",
       "163416                    fenômeno, usando, celular, suicídio, presumido, tardia, ineficient, ausência, condutor, reação   \n",
       "163417                                frent, distância, manter, deixou, via, presença, acessar, observar, outro, veículo   \n",
       "163418  proibida, trafegar, similar, motocicleta, ultrapassagem, indevida, transitar, contramão, incompatível, velocidad   \n",
       "163419                                frent, distância, manter, deixou, via, presença, acessar, observar, outro, veículo   \n",
       "163420              cruzamento, desrespeitar, preferência, dormindo, condutor, faixa, manobra, mudança, álcool, ingestão   \n",
       "\n",
       "                                           normalizado  \n",
       "0                             ausência reação condutor  \n",
       "1                            entrada inopinada pedestr  \n",
       "2                    reação tardia ineficient condutor  \n",
       "3                               velocidad incompatível  \n",
       "4                          acumulo água sobr pavimento  \n",
       "...                                                ...  \n",
       "163416               reação tardia ineficient condutor  \n",
       "163417     acessar via observar presença outro veículo  \n",
       "163418                             transitar contramão  \n",
       "163419  condutor deixou manter distância veículo frent  \n",
       "163420                               condutor dormindo  \n",
       "\n",
       "[163421 rows x 4 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[['Categoria', 'Topic', 'Top Terms', 'normalizado']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Salvando para o PowerBI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.to_csv('./resultados/Data categorizado.csv', index=False, sep=';')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "consult_nlp_vivi",
   "widgets": {}
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
